{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "import pytz\n",
    "import re\n",
    "\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function and global vars definitions\n",
    "def save_csv(df: pl.DataFrame, output_data_path: Path, output_name: str, v: bool = False):\n",
    "    output_path: Path = output_data_path / f\"{output_name}.csv\"\n",
    "    df.write_csv(output_path)\n",
    "    v and print(f\"Saved: {output_path}\")\n",
    "\n",
    "\n",
    "def collocate_on_location(df: pl.DataFrame, collocation_location: str, v: bool = False):\n",
    "    collocated_timestamps: pl.DataFrame = (\n",
    "        df.filter(\n",
    "            pl.col(LOCATION_ID.label) == collocation_location\n",
    "        )\n",
    "        .select(\n",
    "            pl.col(UTC_MINUTE.label)\n",
    "        )\n",
    "    )\n",
    "    result: pl.DataFrame = (\n",
    "        df.join(\n",
    "            collocated_timestamps,\n",
    "            on=UTC_MINUTE.label,\n",
    "            how=\"inner\"\n",
    "        )\n",
    "        .sort(UTC_MINUTE.label)\n",
    "    )\n",
    "    \n",
    "    v and print(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ColumnDefinition:\n",
    "    label: str\n",
    "    dtype: any\n",
    "\n",
    "DATETIME_DTYPE = pl.Datetime(\"ms\")\n",
    "LOCATION_ID = ColumnDefinition(\n",
    "    label=\"location_id\",\n",
    "    dtype=\"string\" # not enforces anywhere\n",
    ")\n",
    "UTC_MINUTE = ColumnDefinition(\n",
    "    label=\"utc_minute\",\n",
    "    dtype=DATETIME_DTYPE\n",
    ")\n",
    "\n",
    "RAW_FMI_BUNDLES_PATH: Path = Path(\"raw/FMI_bundles\")\n",
    "PROCESSED_DATA_PATH: Path = Path(\"processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: SOD_B\n",
      "- Found sensor with id SN081.\n",
      "Saved: processed/SOD_B_1min.csv\n",
      "Processing: HALSSIAAPA_A\n",
      "- Found sensor with id SN122.\n",
      "Saved: processed/HALSSIAAPA_A_1min.csv\n",
      "Processing: SOD_A\n",
      "- Found sensor with id SN122.\n",
      "- Found sensor with id SN081.\n",
      "Saved: processed/SOD_A_1min.csv\n",
      "Saved: processed/ALL_LOCATIONS_1min.csv\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data\n",
    "save_all_locations_1m = True\n",
    "# save_all_locations_1m = False\n",
    "v = True\n",
    "\n",
    "if save_all_locations_1m:\n",
    "    all_dfs = []\n",
    "    # Iterate over each subdirectory (location folder)\n",
    "    for location_dir in RAW_FMI_BUNDLES_PATH.iterdir():\n",
    "        if location_dir.is_dir():\n",
    "            location = location_dir.name\n",
    "            print(f\"Processing: {location}\")\n",
    "\n",
    "            csv_files = list(location_dir.glob(\"*.csv\"))\n",
    "            dfs = []\n",
    "\n",
    "            for file in csv_files:\n",
    "                # Extract sensor ID (e.g., SN122) from filename\n",
    "                match = re.search(r\"SN\\d{3}\", file.name)\n",
    "                sensor_id = match.group(0) if match else \"UNKNOWN\"\n",
    "\n",
    "                print(f\"- Found sensor with id {sensor_id}.\")\n",
    "\n",
    "                df = pl.read_csv(file, try_parse_dates=True)\n",
    "\n",
    "                # Add sensor_id as a column\n",
    "                df = df.with_columns(\n",
    "                    pl.lit(sensor_id).alias(\"sensor_id\")\n",
    "                )\n",
    "\n",
    "                dfs.append(df)\n",
    "\n",
    "            if not dfs:\n",
    "                print(f\"> Skipping {location}, no CSVs found.\")\n",
    "                continue\n",
    "\n",
    "            # Combine all CSVs for the current location\n",
    "            combined = pl.concat(dfs)\n",
    "\n",
    "            # Parse utc to datetime and truncate to 1-minute\n",
    "            combined = combined.with_columns(\n",
    "                pl.col(\"utc\").dt.truncate(\"1m\").cast(UTC_MINUTE.dtype).alias(UTC_MINUTE.label),\n",
    "            )\n",
    "\n",
    "            # Group by minute and sensor_id, then aggregate\n",
    "            aggregated = (\n",
    "                combined\n",
    "                .group_by([UTC_MINUTE.label, \"sensor_id\", \"location_id\"])\n",
    "                .agg([\n",
    "                    pl.col(pl.Float64).mean(),\n",
    "                    pl.len().alias(\"n_measurements\")  # count per group\n",
    "                ])\n",
    "                .sort(\"n_measurements\", UTC_MINUTE.label)\n",
    "            )\n",
    "\n",
    "            aggregated = aggregated.with_columns(\n",
    "                (pl.col(UTC_MINUTE.label).cast(pl.Int64)/1_000).alias(\"unix_epoch\")\n",
    "            )\n",
    "\n",
    "            # Save result\n",
    "            output_path = PROCESSED_DATA_PATH / f\"{location}_1min.csv\"\n",
    "            aggregated.write_csv(output_path)\n",
    "            v and print(f\"Saved: {output_path}\")\n",
    "            del output_path\n",
    "\n",
    "            # create single dataset\n",
    "            all_dfs.append(aggregated)\n",
    "\n",
    "    full_df = pl.concat(all_dfs).sort(UTC_MINUTE.label)\n",
    "    output_path = PROCESSED_DATA_PATH / f\"ALL_LOCATIONS_1min.csv\"\n",
    "    full_df.write_csv(output_path)\n",
    "    v and print(f\"Saved: {output_path}\")\n",
    "    del output_path\n",
    "\n",
    "    assert dt.datetime.fromtimestamp(1585831680.0, tz=pytz.utc) == pytz.utc.localize(dt.datetime.strptime(\"2020-04-02T12:48\", \"%Y-%m-%dT%H:%M\"))\n",
    "    assert dt.datetime.fromtimestamp(1713534120.0, tz=pytz.utc) == pytz.utc.localize(dt.datetime.strptime(\"2024-04-19T13:42\", \"%Y-%m-%dT%H:%M\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collocated_timestamps.shape: (12156, 2)\n",
      "collocated_data.shape: (24312, 24)\n",
      "Collocated dataset has the correct shape.\n",
      "Saved: processed/ALL_LOCATIONS_collocated_1min.csv\n"
     ]
    }
   ],
   "source": [
    "# Collocate full data\n",
    "collocate_full_data = True\n",
    "# collocate_full_data = False\n",
    "v = True\n",
    "\n",
    "if collocate_full_data:\n",
    "    data_file = PROCESSED_DATA_PATH/\"ALL_LOCATIONS_1min.csv\"\n",
    "    df = pl.read_csv(data_file, try_parse_dates=True)\n",
    "    df = df.with_columns([\n",
    "        pl.col(UTC_MINUTE.label).cast(UTC_MINUTE.dtype)\n",
    "    ])\n",
    "\n",
    "    collocated_timestamps = (\n",
    "        df.group_by(UTC_MINUTE.label)\n",
    "        .agg([pl.col(\"sensor_id\").n_unique().alias(\"unique_sensor_count\")])\n",
    "        .filter(pl.col(\"unique_sensor_count\") >= 2)\n",
    "    )\n",
    "\n",
    "    print(\"collocated_timestamps.shape:\", collocated_timestamps.shape)\n",
    "\n",
    "    collocated_data = (\n",
    "        df.join(\n",
    "            collocated_timestamps.select(\n",
    "                pl.col(UTC_MINUTE.label)\n",
    "            ),\n",
    "            on=UTC_MINUTE.label,\n",
    "            how=\"inner\"\n",
    "        )\n",
    "        .sort(UTC_MINUTE.label)\n",
    "    )\n",
    "    # .join(df, on=\"utc_minute\", how=\"inner\").sort(\"utc_minute\")\n",
    "\n",
    "    print(\"collocated_data.shape:\", collocated_data.shape)\n",
    "    assert collocated_data.shape[0] == 2 * collocated_timestamps.shape[0], \"Collocated data is not of size 2 * collocated timestamps\"\n",
    "    print(\"Collocated dataset has the correct shape.\")\n",
    "    # display(collocated_data)\n",
    "    output_path = PROCESSED_DATA_PATH / f\"ALL_LOCATIONS_collocated_1min.csv\"\n",
    "    collocated_data.write_csv(output_path)\n",
    "    v and print(f\"Saved: {output_path}\")\n",
    "    del output_path\n",
    "    # print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data was split between SOD_B and HALSSIAAPA_ correctly.\n",
      "No nulls found.\n"
     ]
    }
   ],
   "source": [
    "# Collocate by location\n",
    "save_collocate_by_location = True\n",
    "# save_collocate_by_location = False\n",
    "v = True\n",
    "\n",
    "if save_collocate_by_location:\n",
    "    data_file = PROCESSED_DATA_PATH/\"ALL_LOCATIONS_collocated_1min.csv\"\n",
    "    df = pl.read_csv(data_file, try_parse_dates=True)\n",
    "    df = df.with_columns([\n",
    "        pl.col(UTC_MINUTE.label).cast(UTC_MINUTE.dtype)\n",
    "    ])\n",
    "\n",
    "    v = False\n",
    "    collocated_SOD_A_SOD_B = collocate_on_location(df, \"SOD_B\", v=v)\n",
    "    collocated_SOD_A_HALSSIAAPA_A = collocate_on_location(df, \"HALSSIAAPA_A\", v=v)\n",
    "    assert collocated_SOD_A_SOD_B.shape[0] + collocated_SOD_A_HALSSIAAPA_A.shape[0] == 24312, \"Collocated set 1 + collocated set 2 does not equal total number of collocated data points\"\n",
    "    print(\"Data was split between SOD_B and HALSSIAAPA_ correctly.\")\n",
    "    save_csv(collocated_SOD_A_SOD_B, PROCESSED_DATA_PATH, \"SOD_A_SOD_B_collocated_1min\")\n",
    "    save_csv(collocated_SOD_A_HALSSIAAPA_A, PROCESSED_DATA_PATH, \"SOD_A_HALSSIAAPA_A_collocated_1min\")\n",
    "\n",
    "# check for null values\n",
    "assert collocated_SOD_A_SOD_B.null_count().sum_horizontal().item() == 0, \"Nulls found in collocated_SOD_A_SOD_B\"\n",
    "assert collocated_SOD_A_HALSSIAAPA_A.null_count().sum_horizontal().item() == 0, \"Nulls found in collocated_SOD_A_HALSSIAAPA_A\"\n",
    "print(\"No nulls found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysun_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
